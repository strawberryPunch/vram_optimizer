import torch
import gc
import time

class VRAMCleaner:
    """VRAM ì •ë¦¬ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, clear_mode="Standard"):
        self.clear_mode = clear_mode
    
    def is_cuda_available(self):
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return torch.cuda.is_available()
    
    def get_allocated_memory(self):
        """í˜„ì¬ í• ë‹¹ëœ VRAM ë©”ëª¨ë¦¬ í¬ê¸° (MB)"""
        if self.is_cuda_available():
            return torch.cuda.memory_allocated() / 1024**2
        return 0
    
    def perform_cleanup(self):
        """VRAM ì •ë¦¬ ì‹¤í–‰"""
        if not self.is_cuda_available():
            return {
                'success': False,
                'error': 'CUDA ì‚¬ìš© ë¶ˆê°€',
                'before': 0,
                'after': 0,
                'cleared': 0
            }
        
        try:
            before = self.get_allocated_memory()
            
            # ê¸°ë³¸ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            
            # Aggressive ëª¨ë“œì¼ ë•Œ ì¶”ê°€ ì •ë¦¬
            if self.clear_mode == "Aggressive":
                gc.collect()
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
            
            after = self.get_allocated_memory()
            cleared = before - after
            
            return {
                'success': True,
                'before': before,
                'after': after,
                'cleared': cleared,
                'mode': self.clear_mode
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'before': 0,
                'after': 0,
                'cleared': 0
            }
    
    def log_cleanup_progress(self, current_time):
        """ì •ë¦¬ ì§„í–‰ ìƒí™© ë¡œê·¸"""
        print(f"âš¡ [{current_time}] VRAM ì •ë¦¬ ì§„í–‰... ({self.clear_mode} ëª¨ë“œ)")
        print(f"   ğŸ”§ torch.cuda.empty_cache() ì‹¤í–‰ì¤‘...")
        
        if self.clear_mode == "Aggressive":
            print(f"   ğŸ”§ gc.collect() ì‹¤í–‰ì¤‘...")
            if hasattr(torch.cuda, 'synchronize'):
                print(f"   ğŸ”§ torch.cuda.synchronize() ì‹¤í–‰ì¤‘...")
    
    def log_cleanup_result(self, result, current_time):
        """ì •ë¦¬ ê²°ê³¼ ë¡œê·¸"""
        if result['success']:
            if result['cleared'] > 0:
                print(f"ğŸ‰ [{current_time}] VRAM ì •ë¦¬ ì„±ê³µ! {result['before']:.1f}MB â†’ {result['after']:.1f}MB (í•´ì œ: {result['cleared']:.1f}MB)")
            else:
                print(f"âœ¨ [{current_time}] ì´ë¯¸ ìµœì í™”ëœ ìƒíƒœ (í˜„ì¬: {result['after']:.1f}MB)")
        else:
            print(f"âŒ [{current_time}] VRAM ì •ë¦¬ ì‹¤íŒ¨: {result['error']}")
    
    def generate_ui_message(self, result, current_time, execution_count):
        """UI ë©”ì‹œì§€ ìƒì„±"""
        execution_info = f"[ì‹¤í–‰#{execution_count}] "
        
        if result['success']:
            if result['cleared'] > 0:
                return f"ğŸ‰ {execution_info}[{current_time}] VRAM ì •ë¦¬ ì™„ë£Œ! í•´ì œ: {result['cleared']:.1f}MB"
            else:
                return f"âœ¨ {execution_info}[{current_time}] ì´ë¯¸ ìµœì í™”ëœ ìƒíƒœ ({result['after']:.1f}MB)"
        else:
            return f"âŒ {execution_info}[{current_time}] {result['error']}"