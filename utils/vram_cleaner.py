import torch
import gc
import time

class VRAMCleaner:
    """VRAM ì •ë¦¬ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, clear_mode="Standard"):
        self.clear_mode = clear_mode
    
    def is_cuda_available(self):
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return torch.cuda.is_available()
    
    def get_allocated_memory(self):
        """í˜„ì¬ í• ë‹¹ëœ VRAM ë©”ëª¨ë¦¬ í¬ê¸° (MB)"""
        if self.is_cuda_available():
            return torch.cuda.memory_allocated() / 1024**2
        return 0
    
    def perform_cleanup(self):
        """VRAM ì •ë¦¬ ì‹¤í–‰"""
        if not self.is_cuda_available():
            return {
                'success': False,
                'error': 'CUDA ì‚¬ìš© ë¶ˆê°€',
                'before': 0,
                'after': 0,
                'cleared': 0
            }
        
        try:
            before = self.get_allocated_memory()
            
            # ê¸°ë³¸ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            
            # Aggressive ëª¨ë“œì¼ ë•Œ ì¶”ê°€ ì •ë¦¬
            if self.clear_mode == "Aggressive":
                gc.collect()
                if hasattr(torch.cuda, 'synchronize'):
                    torch.cuda.synchronize()
            
            after = self.get_allocated_memory()
            cleared = before - after
            
            return {
                'success': True,
                'before': before,
                'after': after,
                'cleared': cleared,
                'mode': self.clear_mode
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'before': 0,
                'after': 0,
                'cleared': 0
            }
    
    def log_cleanup_progress(self, current_time):
        """Log cleanup progress"""
        print(f"âš¡ [{current_time}] VRAM cleanup in progress... ({self.clear_mode} mode)")
        print(f"   ğŸ”§ Executing torch.cuda.empty_cache()...")
        
        if self.clear_mode == "Aggressive":
            print(f"   ğŸ”§ Executing gc.collect()...")
            if hasattr(torch.cuda, 'synchronize'):
                print(f"   ğŸ”§ Executing torch.cuda.synchronize()...")
    
    def log_cleanup_result(self, result, current_time):
        """Log cleanup result"""
        if result['success']:
            if result['cleared'] > 0:
                print(f"ğŸ‰ [{current_time}] VRAM cleanup successful! {result['before']:.1f}MB â†’ {result['after']:.1f}MB (freed: {result['cleared']:.1f}MB)")
            else:
                print(f"âœ¨ [{current_time}] Already optimized (current: {result['after']:.1f}MB)")
        else:
            print(f"âŒ [{current_time}] VRAM cleanup failed: {result['error']}")
    
    def generate_ui_message(self, result, current_time, execution_count):
        """Generate UI message"""
        execution_info = f"[Execution#{execution_count}] "
        
        if result['success']:
            if result['cleared'] > 0:
                return f"ğŸ‰ {execution_info}[{current_time}] VRAM cleanup completed! Freed: {result['cleared']:.1f}MB"
            else:
                return f"âœ¨ {execution_info}[{current_time}] Already optimized ({result['after']:.1f}MB)"
        else:
            return f"âŒ {execution_info}[{current_time}] {result['error']}"